---
layout: post
title:  AI in college
date:   2025-07-10
description: 
tags: 
categories:

---

This story is going to end with you getting your ass kicked. (In the story, I mean, not in real life. I hope.) 
> Imagine that, instead of going to college, you enroll in a four-year program at a sports training institute. It doesn’t matter what your sport is. Mixed martial arts will be a vivid example, but it can be anything---tennis, basketball, running, or whatever you like. The purpose of this four-year program is to teach, train, and prepare you for a career competing professionally. Once you finish your program, you will---if everything goes according to plan—be hired by a team or a league. They will pay you and arrange for you to compete. 

You see the analogy with going to college and then getting a job.

> While you are at this sports training institute, you have to summit records of the training and practice that you do on your own, and sometimes you have to practice, train, and compete in front of trainers and coaches. You, however, thinking that you are clever and because you have an identical twin, don’t actually do any training, practicing, or competing during your four years. You make up the records of the training that you were supposed to do on your own, and you send your twin for all of the in-person events.

> Because of your deceit, as the end of your four years approaches, you are the highest ranked mixed martial arts fighter---or whatever your sport is---at the training institute. The top four MMA leagues in the world desperately want you. (And the other leagues know that they have no chance of getting you.) You accept an offer, and your twin, who has no interest in competing professionally, goes back to live with your parents. A week later, you step into the cage for your first professional MMA fight, and you get your ass kicked.
 
I could add more details to the story to address this or that thought about extending the deceit, but you get the idea. You spent four years supposedly preparing to compete in some sport, but when you finish, you’re completely unprepared to do so. And so it goes with using A.I. tools in college.[^1]

[^1]: Of course, the sophisticated use of A.I. in business, medicine, science, and other fields is a skill that many people will need and want to develop. My philosophy courses (and many others) are not designed to impart those skills, however, and students won’t develop them by putting “write a six paragraph essay on the pros and cons of the categorical imperative” into ChatGPT.
 
The moral of the story about you getting your ass kicked is that you have to be prepared when you finish college. To one degree or another, this has always been true, but, ironically, the rise of A.I. is making it even more pressing. If we’re being optimistic, we can hope that A.I. will help us create a [wealthier and healthier society](https://www.darioamodei.com/essay/machines-of-loving-grace), one in which people will work less, have more time to pursue their interests and hobbies, and spend more time with friends and family.[^2] 
Before that happens, though, there will have to be a “realignment” to our economy. Because A.I. will do the work more cheaply and efficiently, a lot of jobs are going to disappear and [a lot of workers are going to be laid off or never hired in the first place](https://www.nytimes.com/2025/05/19/opinion/linkedin-ai-entry-level-jobs.html) .


[^2]: The optimistic scenario is that, after an economic adjustment, A.I. produces significant benefits for society. But this isn’t guaranteed. Consider the lesson of globalization. In principle, a globalized economy is superior to one in which every country’s economy is isolated from all others, but globalization can still inflict harms without enough offsetting benefits. Speaking about globalization, the Nobel Prize winning economist Joseph Stiglitz makes a point that is also resonant for the A.I. driven economy: [“The argument was always that the winners could compensate the losers. But the winners never do.”](https://www.nytimes.com/2015/05/18/business/a-decade-later-loss-of-maytag-factory-still-resonates.html)


So, what does this mean for undergraduates today? It means that you need to do everything possible to develop and improve your academic and intellectual skills, especially (although not only) those that are not domain specific: critical thinking and reasoning, problem solving, reading comprehension, clear and grammatical writing (with, if possible, a compelling writing style), and confidence speaking publicly. Such skills will give you the best chance to stay ahead of others in the race for dwindling economic rewards. They will also put you in the best position to adapt to new challenges and job opportunities in a changing economic landscape. And they will distinguish you from what, for the time being, A.I. can do. Maybe in the future, A.I. will be able to mimic some of them, but even then, you want to be a mammal that has these skills, not one that doesn’t.

How do you develop these skills? This question deserves a long answer, which I’m not going to give here. (But if you’re one of my students, you can ask me if you like.) This is the short answer: take challenging courses, especially ones where you have access to the professor and will receive (or can solicit) constructive feedback on your work. AND!!! don’t cheat or take shortcuts with A.I. The only way to develop these academic and intellectual skills is to go through the process yourself. You won’t always have success right away---in fact, you shouldn’t---but over time you’ll improve. And don’t get too discouraged by mistakes and failures. They’re part of the process. 


The final point that I want to make is that it may seem that using A.I. is an undetectable way of cheating. It's not, though. Like cheating in general, sometimes students will get away with it but sometimes they won't.

It’s well known (by professors and, I think, by students) that [ChatGPT and other large language models have a recognizable style](https://seanjkernan.substack.com/p/13-signs-you-used-chatgpt-to-write). This, perhaps, can be adjusted to fool the reader, but my preferred way of catching students is different. 
Generally, I provide all of the materials that the students in my courses use (i.e., the articles, chapters, books, notes, and videos). I pretty much have all of these materials memorized, and so I have a very good idea about what I can expect my students to know. For instance, if I assign an introductory chapter on the problem of free will and have my students write an essay using it (and only it) as a resource, then I know the range of issues and terminology that (I hope) they understand&mdash;_determined_, _caused_, _free_, _arbitrary_, and the like. 

Therein lies the catch. When doing your work for you, ChatGPT is liable to slip in terms that, perhaps, make sense but shouldn’t be there, and a student who doesn’t know what is in the assigned chapter isn’t going to realize it. So, when an essay refers to advanced concepts or ideas with which I wouldn’t expect a student to be familiar or when the page references are wrong, then the student will have to explain the essay to me. This isn’t a great use of my time and I don’t revel in giving harsh penalties when it’s clear that a student doesn’t understand the content of his or her own essay, but ask ChatGPT what _deterrence_ means.

